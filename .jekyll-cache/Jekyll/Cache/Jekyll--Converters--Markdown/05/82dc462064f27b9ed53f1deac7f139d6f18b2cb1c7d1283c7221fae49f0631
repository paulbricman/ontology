I"R<p>Reducing moral uncertainty is instrumental to doing the right thing. After all, it is the process of getting to know what that thing is, and so making it infinitely more likely to be done. The alternative? Going all in on the few pockets of the moral realism roulette whose names we’ve picked up along our civilizational childhood. I wouldn’t take this bet.</p>

<p>And so I take a step back, how could one reduce uncertainty about such an immaterial object? That’s the focus of the first volume of Elements of Computational Philosophy. In brief, the book pursues an operationalization of metaphysical truth-seeking which provably boils down to a particular computable function. Unfortunately, this line of work is nowhere near the point of providing a complete and reflexively robust operationalization of truth. Exciting progress has been made on one tenth of the puzzle, yes, but there is so much to be done.</p>

<p>And so I take a step back, how could one make progress on an operationalization of metaphysical truth-seeking? Things become more contingent from here on. Half a year ago, I saw four approaches.</p>

<p>First, I could continue with independent research, coordinating with external collaborators in favorable contexts. While the academic freedom would be second to none, this approach would lack the organizational capacity required for genuinely tackling such questions.</p>

<p>Second, I could work my way through organizations focused on related challenges, technical alignment research being especially close in ambitions. While the capacity would be second to none, this approach would involve defaulting this work to particular counterproductive frames, and so having limited effective academic freedom beyond senior researchers.</p>

<p>Third, I could start a non-profit, again. While this would seemingly restore the autonomy of independent research, capacity would still hinge on a legible compatibility with particular counterproductive frames when it comes to fundraising.</p>

<p>Fourth, I could start a for-profit, for the first time. This starts getting interesting, as the output responsible for producing legible value need not be exactly the same as the output responsible for advancing this line of work, as in the case of non-profits. There is some degree of freedom to pursue in-house research which is not directly productizable. However, it does bring its own challenges related to simultaneously aligning product to both market needs and this research North Star of operationalizing morality and baking it into autonomous systems. That said, even if navigating these challenges in the highly volatile market that is AI is non-trivial, the outlook on-net seems especially promising.</p>

<p>And so I take a step back, how could one make work on quantifying specific properties of AI systems commercially sustainable? It turns out that there are multiple AI properties whose attested measurement and management is of near-future interest, such as dual-use capabilities. This brings us to a theory of change for a commercial project focused on offerings related to tracking and sculpting properties of deployed models.</p>

<p>First, there is a shared focus on reliable evaluation infrastructure. If evaluations are abstracted away as maps between model parameter, activations, logits, gradients, or input-output samples, and numerical values of interest, then different evaluations start having a lot in common. You want measurements with test-retest reliability, guarantees on bounds, efficiency and scalability, results attestation, etc. You want reliable measurements of AI systems, in general. An automated proof on the moral defensibility of a model could end up having a lot in common, conceptually, with one about the correctness of programs generated in a particular context.</p>

<p>Second, there is a shared focus on distribution of measurement instruments. This involves everything from precedents of coordinating with institutions and AI labs, all the way to having artifacts readily available across the cloud hyperscalers. In other words, meaningful operationalizations need to be available in the real-world if they are to end up shaping the developmental psychology of AI systems. Imagine authorities having the capacity for widespread attestation of moral defensibility in friendshored models.</p>

<p>Third, the consumption of legibly useful measurements and auxiliary infrastructure by AI labs, third-party integrators, institutions, etc. could help fund research into these more charged, loaded operationalizations, as well as loop back into general infrastructure. That said, the tractability and exact shape of the product and market are beyond the scope of this note.</p>

<p>Fourth, even if this direction turns out to not be what ought to be done, there are a number of side effects which might still make it worthwhile. For instance, there’s the attempt to mitigate misuse and loss of control through the management of dual-use capabilities. There’s effort towards reliable measurements of AI systems in a broader sense. In other words, there appears to be a reasonable amount of defensibility to this course of action even when deferring prioritization entirely to global priorities researchers.</p>

<p>There are many other branches left behind in this backward chaining. For instance, effective communication about this line of work might also be high-leverage, as might more actively engaging with proponents of other frames. Alternatively, one might argue that another for-profit angle aligned with reducing moral uncertainty is automated reasoning more broadly. Branches need to be pruned at some point, for better or worse.</p>
:ET